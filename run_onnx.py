#!/usr/bin/env python3
"""
è¿è¡ŒONNXä¼˜åŒ–è¿‡çš„æ¨¡å‹ - ç®€å•å¯é 
"""

import os
import time
import numpy as np
import onnxruntime as ort

class ONNXRunner:
    def __init__(self, model_path):
        """åˆå§‹åŒ–ONNXæ¨ç†å™¨"""
        self.model_path = model_path
        
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°ONNXæ¨¡å‹æ–‡ä»¶: {model_path}")
        
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½ONNXæ¨¡å‹"""
        print(f"æ­£åœ¨åŠ è½½ONNXæ¨¡å‹: {self.model_path}")
        
        # åˆ›å»ºæ¨ç†ä¼šè¯ï¼Œä¼˜å…ˆä½¿ç”¨CUDA
        providers = [
            ('CUDAExecutionProvider', {
                'device_id': 0,
                'arena_extend_strategy': 'kNextPowerOfTwo',
                'gpu_mem_limit': 2 * 1024 * 1024 * 1024,  # 2GB
                'cudnn_conv_algo_search': 'EXHAUSTIVE',
                'do_copy_in_default_stream': True,
            }),
            'CPUExecutionProvider'
        ]
        
        try:
            self.session = ort.InferenceSession(self.model_path, providers=providers)
            print(f"âœ… ONNXæ¨¡å‹åŠ è½½æˆåŠŸ!")
            print(f"   æ–‡ä»¶å¤§å°: {os.path.getsize(self.model_path) / 1024 / 1024:.2f} MB")
            print(f"   æä¾›ç¨‹åº: {self.session.get_providers()}")
        except Exception as e:
            print(f"âš ï¸  CUDAåŠ è½½å¤±è´¥ï¼Œä½¿ç”¨CPU: {e}")
            self.session = ort.InferenceSession(self.model_path, providers=['CPUExecutionProvider'])
            print(f"âœ… ONNXæ¨¡å‹åŠ è½½æˆåŠŸ (CPU)!")
    
    def predict(self, input_data):
        """æ‰§è¡Œæ¨ç†é¢„æµ‹"""
        if not isinstance(input_data, np.ndarray):
            input_data = np.array(input_data)
        
        # ç¡®ä¿æ•°æ®ç±»å‹å’Œå½¢çŠ¶æ­£ç¡®
        if input_data.dtype != np.float32:
            input_data = input_data.astype(np.float32)
        
        expected_shape = (1, 30, 33)  # batch_size, seq_len, features
        if input_data.shape != expected_shape:
            if input_data.ndim == 2:  # å¦‚æœæ˜¯ (30, 33)ï¼Œæ·»åŠ batchç»´åº¦
                input_data = input_data.reshape(1, 30, 33)
            else:
                raise ValueError(f"è¾“å…¥å½¢çŠ¶é”™è¯¯ï¼ŒæœŸæœ› {expected_shape}ï¼Œå¾—åˆ° {input_data.shape}")
        
        # æ‰§è¡Œæ¨ç†
        start_time = time.time()
        result = self.session.run(['up_probability'], {'input': input_data})
        end_time = time.time()
        
        inference_time = end_time - start_time
        prediction = result[0][0]  # è·å–é¢„æµ‹æ¦‚ç‡
        
        return prediction, inference_time
    
    def predict_batch(self, input_data_list):
        """æ‰¹é‡é¢„æµ‹"""
        results = []
        total_time = 0
        
        for i, data in enumerate(input_data_list):
            prediction, inference_time = self.predict(data)
            results.append(prediction)
            total_time += inference_time
            
            if i == 0:  # ç¬¬ä¸€æ¬¡é¢„æµ‹é€šå¸¸è¾ƒæ…¢ï¼ˆåŒ…å«åˆå§‹åŒ–ï¼‰
                print(f"ç¬¬ä¸€æ¬¡æ¨ç†æ—¶é—´: {inference_time*1000:.2f} ms")
        
        avg_time = total_time / len(input_data_list)
        return results, avg_time

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ONNXæ¨¡å‹"""
    print("=" * 60)
    print("ONNXæ¨¡å‹æ¨ç†æ¼”ç¤º")
    print("=" * 60)
    
    # æ£€æŸ¥å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶
    optimized_model = "informer_cls.optimized.onnx"
    original_model = "informer_cls.fp32.onnx"
    
    model_path = None
    if os.path.exists(optimized_model):
        model_path = optimized_model
        print(f"ä½¿ç”¨ä¼˜åŒ–æ¨¡å‹: {optimized_model}")
    elif os.path.exists(original_model):
        model_path = original_model
        print(f"ä½¿ç”¨åŸå§‹æ¨¡å‹: {original_model}")
    else:
        print("âŒ æ‰¾ä¸åˆ°ONNXæ¨¡å‹æ–‡ä»¶!")
        print("è¯·å…ˆè¿è¡Œ: python export_to_onnx.py")
        return
    
    try:
        # åˆå§‹åŒ–æ¨ç†å™¨
        runner = ONNXRunner(model_path)
        
        # 1. å•æ¬¡é¢„æµ‹
        print(f"\nğŸ“Š å•æ¬¡é¢„æµ‹æµ‹è¯•:")
        input_data = np.random.randn(1, 30, 33).astype(np.float32)
        prediction, inference_time = runner.predict(input_data)
        
        print(f"   è¾“å…¥å½¢çŠ¶: {input_data.shape}")
        print(f"   æ¨ç†æ—¶é—´: {inference_time*1000:.2f} ms")
        print(f"   é¢„æµ‹ç»“æœ: {prediction:.6f}")
        print(f"   é¢„æµ‹ç±»åˆ«: {'ä¸Šæ¶¨' if prediction > 0.5 else 'ä¸‹è·Œ'}")
        
        # 2. æ‰¹é‡é¢„æµ‹
        print(f"\nğŸ“ˆ æ‰¹é‡é¢„æµ‹æµ‹è¯•:")
        batch_size = 5
        batch_data = [np.random.randn(1, 30, 33).astype(np.float32) for _ in range(batch_size)]
        
        predictions, avg_time = runner.predict_batch(batch_data)
        
        print(f"   æ‰¹é‡å¤§å°: {batch_size}")
        print(f"   å¹³å‡æ¨ç†æ—¶é—´: {avg_time*1000:.2f} ms")
        print(f"   æ€»ååé‡: {batch_size/avg_time:.1f} samples/s")
        print(f"   é¢„æµ‹ç»“æœ: {[f'{p:.4f}' for p in predictions]}")
        
        # 3. æ€§èƒ½å¯¹æ¯”
        print(f"\nâš¡ æ€§èƒ½æ€»ç»“:")
        print(f"   å•æ¬¡æ¨ç†: {inference_time*1000:.2f} ms")
        print(f"   å¹³å‡æ¨ç†: {avg_time*1000:.2f} ms")
        print(f"   ååé‡: {1/avg_time:.1f} samples/s")
        
        # 4. ä¸åŒbatch sizeæµ‹è¯•
        print(f"\nğŸ”¢ ä¸åŒbatch sizeæ€§èƒ½æµ‹è¯•:")
        for bs in [1, 2, 4, 8, 16]:
            try:
                batch_data = [np.random.randn(1, 30, 33).astype(np.float32) for _ in range(bs)]
                predictions, avg_time = runner.predict_batch(batch_data)
                throughput = bs / avg_time
                print(f"   Batch {bs:2d}: {avg_time*1000:6.2f} ms, {throughput:6.1f} samples/s")
            except Exception as e:
                print(f"   Batch {bs:2d}: å¤±è´¥ - {e}")
        
        print(f"\nğŸ‰ ONNXæ¨ç†æ¼”ç¤ºå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def predict_single(input_data):
    """ä¾¿æ·å‡½æ•°ï¼šå•æ¬¡é¢„æµ‹"""
    model_path = "informer_cls.optimized.onnx"
    if not os.path.exists(model_path):
        model_path = "informer_cls.fp32.onnx"
    
    runner = ONNXRunner(model_path)
    prediction, inference_time = runner.predict(input_data)
    return prediction, inference_time

def predict_from_array(data_array):
    """ä»numpyæ•°ç»„é¢„æµ‹"""
    if data_array.shape == (30, 33):
        # æ·»åŠ batchç»´åº¦
        input_data = data_array.reshape(1, 30, 33)
    elif data_array.shape == (1, 30, 33):
        input_data = data_array
    else:
        raise ValueError(f"è¾“å…¥å½¢çŠ¶é”™è¯¯ï¼ŒæœŸæœ› (30, 33) æˆ– (1, 30, 33)ï¼Œå¾—åˆ° {data_array.shape}")
    
    prediction, inference_time = predict_single(input_data)
    return prediction, inference_time

if __name__ == '__main__':
    main()
