#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆå››ç§æ¨¡å‹æ¨ªå‘å¯¹æ¯”æµ‹è¯•è„šæœ¬
åŠŸèƒ½ï¼š
1. åŸå§‹æ¨¡å‹ (PyTorch)
2. TensorRT FP32åçš„åŸå§‹æ¨¡å‹
3. è’¸é¦å‰ªæè¿‡åçš„å­¦ç”Ÿæ¨¡å‹ (PyTorch)
4. è’¸é¦å‰ªæè¿‡åå†TensorRTç¼–è¯‘ä¼˜åŒ–çš„å­¦ç”Ÿæ¨¡å‹

å¢å¼ºåŠŸèƒ½ï¼š
- è‡ªåŠ¨æ£€æµ‹æ¨¡å‹æ–‡ä»¶å­˜åœ¨æ€§
- è¯¦ç»†çš„æ€§èƒ½åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆ
- æ”¯æŒå¤šç§ç²¾åº¦æ¨¡å¼å¯¹æ¯”
- ç”ŸæˆHTMLæ ¼å¼çš„å¯¹æ¯”æŠ¥å‘Š
- å†…å­˜ä½¿ç”¨æƒ…å†µç›‘æ§
- é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶
"""

import torch
import numpy as np
import time
import os
import warnings
import json
import gc
import psutil
from datetime import datetime
from pathlib import Path
import argparse
warnings.filterwarnings('ignore')

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from InformerClassification import InformerClassification, InformerConfig, create_informer_classification_model
from knowledge_distillation_training import StudentInformerClassification, StudentInformerConfig

class ModelComparisonTester:
    """å¢å¼ºç‰ˆæ¨¡å‹å¯¹æ¯”æµ‹è¯•å™¨"""
    
    def __init__(self, device='cuda', num_runs=200, save_report=True):
        self.device = device if torch.cuda.is_available() else 'cpu'
        self.num_runs = num_runs
        self.save_report = save_report
        self.results = {}
        self.start_time = datetime.now()
        
        # æ¨¡å‹æ–‡ä»¶è·¯å¾„é…ç½®
        self.model_paths = {
            'teacher_pytorch': 'checkpoints/bitcoin_mega_classification/mega_best_model.pth',
            'student_pytorch': 'checkpoints/student_distillation/student_best_model.pth',
            'teacher_onnx': 'informer_cls.fp32.onnx',
            'student_onnx': 'student_model.fp32.onnx',
            'teacher_trt_fp32': 'informer_cls.trt.fp32.engine',
            'student_trt_fp32': 'student_model.trt.fp32.engine',
            'student_trt_fp16': 'student_model.trt.fp16.engine'
        }
        
        print(f"ğŸš€ å¢å¼ºç‰ˆæ¨¡å‹å¯¹æ¯”æµ‹è¯•å™¨åˆå§‹åŒ–")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   æµ‹è¯•æ¬¡æ•°: {self.num_runs}")
        print(f"   æŠ¥å‘Šä¿å­˜: {'æ˜¯' if self.save_report else 'å¦'}")
        print("=" * 80)

    def check_model_files(self):
        """æ£€æŸ¥æ‰€æœ‰æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        print("\nğŸ“‹ æ£€æŸ¥æ¨¡å‹æ–‡ä»¶...")
        missing_files = []
        
        for model_name, file_path in self.model_paths.items():
            if os.path.exists(file_path):
                file_size = os.path.getsize(file_path) / 1024 / 1024
                print(f"   âœ… {model_name}: {file_path} ({file_size:.2f} MB)")
            else:
                print(f"   âŒ {model_name}: {file_path} (æ–‡ä»¶ä¸å­˜åœ¨)")
                missing_files.append((model_name, file_path))
        
        if missing_files:
            print(f"\nâš ï¸  ç¼ºå¤± {len(missing_files)} ä¸ªæ¨¡å‹æ–‡ä»¶:")
            for name, path in missing_files:
                print(f"   - {name}: {path}")
            return False
        
        print("   âœ… æ‰€æœ‰æ¨¡å‹æ–‡ä»¶æ£€æŸ¥å®Œæˆ")
        return True

    def get_memory_usage(self):
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if self.device == 'cuda':
            gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024
            gpu_max_memory = torch.cuda.max_memory_allocated() / 1024 / 1024
            return {
                'gpu_current': gpu_memory,
                'gpu_max': gpu_max_memory,
                'cpu_percent': psutil.virtual_memory().percent
            }
        else:
            return {
                'cpu_percent': psutil.virtual_memory().percent,
                'cpu_available': psutil.virtual_memory().available / 1024 / 1024 / 1024
            }

    def clear_memory(self):
        """æ¸…ç†å†…å­˜"""
        if self.device == 'cuda':
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        gc.collect()

    def load_teacher_model(self):
        """åŠ è½½åŸå§‹æ•™å¸ˆæ¨¡å‹"""
        print("\nğŸ“š åŠ è½½åŸå§‹æ•™å¸ˆæ¨¡å‹...")
        teacher_model_path = self.model_paths['teacher_pytorch']
        
        if not os.path.exists(teacher_model_path):
            print(f"âŒ æ•™å¸ˆæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {teacher_model_path}")
            return None
        
        try:
            # åŠ è½½æ£€æŸ¥ç‚¹
            checkpoint = torch.load(teacher_model_path, map_location=self.device)
            config_dict = checkpoint['config']
            
            # åˆ›å»ºé…ç½®
            teacher_config = InformerConfig()
            for key, value in config_dict.items():
                if hasattr(teacher_config, key):
                    setattr(teacher_config, key, value)
            
            # åˆ›å»ºæ¨¡å‹
            teacher_model, _ = create_informer_classification_model(teacher_config)
            teacher_model = teacher_model.to(self.device)
            
            # å¤„ç†åŠ¨æ€æŠ•å½±å±‚
            model_state_dict = checkpoint['model_state_dict']
            if 'projection.weight' in model_state_dict:
                dummy_input = torch.randn(1, teacher_config.seq_len, teacher_config.enc_in).to(self.device)
                with torch.no_grad():
                    _ = teacher_model(dummy_input)
            
            teacher_model.load_state_dict(model_state_dict, strict=False)
            teacher_model.eval()
            
            # è®¡ç®—å‚æ•°é‡
            total_params = sum(p.numel() for p in teacher_model.parameters())
            print(f"   âœ… æ•™å¸ˆæ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"   å‚æ•°é‡: {total_params:,} ({total_params/1_000_000:.1f}M)")
            
            return teacher_model
            
        except Exception as e:
            print(f"âŒ æ•™å¸ˆæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return None

    def load_student_model(self):
        """åŠ è½½å­¦ç”Ÿæ¨¡å‹"""
        print("\nğŸ“ åŠ è½½å­¦ç”Ÿæ¨¡å‹...")
        student_model_path = self.model_paths['student_pytorch']
        
        if not os.path.exists(student_model_path):
            print(f"âŒ å­¦ç”Ÿæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {student_model_path}")
            return None
        
        try:
            # æ¸…ç†GPUå†…å­˜
            self.clear_memory()
            
            # åŠ è½½æ£€æŸ¥ç‚¹ - ä½¿ç”¨CPUåŠ è½½é¿å…CUDAé”™è¯¯
            checkpoint = torch.load(student_model_path, map_location='cpu')
            config_dict = checkpoint['config']
            
            # åˆ›å»ºå­¦ç”Ÿé…ç½®
            student_config = StudentInformerConfig()
            for key, value in config_dict.items():
                if hasattr(student_config, key):
                    setattr(student_config, key, value)
            
            # åˆ›å»ºå­¦ç”Ÿæ¨¡å‹
            student_model = StudentInformerClassification(student_config)
            
            # å¤„ç†åŠ¨æ€æŠ•å½±å±‚ï¼ˆåœ¨CPUä¸Šï¼‰
            model_state_dict = checkpoint['model_state_dict']
            if 'projection.weight' in model_state_dict:
                dummy_input = torch.randn(1, student_config.seq_len, student_config.enc_in)
                with torch.no_grad():
                    _ = student_model(dummy_input)
            
            # åŠ è½½æƒé‡
            student_model.load_state_dict(model_state_dict, strict=False)
            student_model.eval()
            
            # ç§»åŠ¨åˆ°GPUï¼ˆå¦‚æœéœ€è¦ï¼‰
            if self.device == 'cuda':
                student_model = student_model.to(self.device)
                self.clear_memory()
        
            # è®¡ç®—å‚æ•°é‡
            total_params = sum(p.numel() for p in student_model.parameters())
            print(f"   âœ… å­¦ç”Ÿæ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"   å‚æ•°é‡: {total_params:,} ({total_params/1_000_000:.1f}M)")
            
            return student_model
            
        except Exception as e:
            print(f"âŒ å­¦ç”Ÿæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.clear_memory()
            return None

    def test_pytorch_model(self, model, test_input, model_name):
        """æµ‹è¯•PyTorchæ¨¡å‹æ¨ç†å»¶è¿Ÿ"""
        print(f"\nğŸ§ª æµ‹è¯• {model_name} (PyTorch)...")
        
        # é¢„çƒ­
        with torch.no_grad():
            for _ in range(10):
                _ = model(test_input)
        
        # ç²¾ç¡®æµ‹é‡
        times = []
        outputs = []
        
        with torch.no_grad():
            for i in range(self.num_runs):
                if torch.cuda.is_available():
                    torch.cuda.synchronize()
                
                start = time.time()
                output = model(test_input)
                up_prob = output['up_probability']
                
                if torch.cuda.is_available():
                    torch.cuda.synchronize()
                
                end = time.time()
                times.append((end - start) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
                outputs.append(up_prob[0].item())
        
        times_array = np.array(times)
        outputs_array = np.array(outputs)
        
        result = {
            'avg_time': np.mean(times_array),
            'std_time': np.std(times_array),
            'min_time': np.min(times_array),
            'max_time': np.max(times_array),
            'output': np.mean(outputs_array),
            'output_std': np.std(outputs_array),
            'times': times_array,
            'memory_usage': self.get_memory_usage()
        }
        
        print(f"   âœ… {model_name} æµ‹è¯•å®Œæˆ")
        print(f"   å¹³å‡æ¨ç†æ—¶é—´: {result['avg_time']:.3f} Â± {result['std_time']:.3f} ms")
        print(f"   æ—¶é—´èŒƒå›´: {result['min_time']:.3f} - {result['max_time']:.3f} ms")
        print(f"   è¾“å‡ºå€¼: {result['output']:.6f} Â± {result['output_std']:.6f}")
        
        return result

    def test_onnx_model(self, onnx_path, model_name):
        """æµ‹è¯•ONNXæ¨¡å‹æ¨ç†æ€§èƒ½"""
        import onnxruntime as ort
        
        print(f"\nğŸ”§ æµ‹è¯• {model_name} (ONNX)...")
        
        if not os.path.exists(onnx_path):
            print(f"âŒ ONNXæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {onnx_path}")
            return None
        
        try:
            # åˆ›å»ºONNX Runtimeä¼šè¯
            providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if torch.cuda.is_available() else ['CPUExecutionProvider']
            session = ort.InferenceSession(onnx_path, providers=providers)
            
            # è·å–è¾“å…¥è¾“å‡ºä¿¡æ¯
            input_name = session.get_inputs()[0].name
            output_name = session.get_outputs()[0].name
            input_shape = session.get_inputs()[0].shape
            
            print(f"   è¾“å…¥åç§°: {input_name}, å½¢çŠ¶: {input_shape}")
            print(f"   è¾“å‡ºåç§°: {output_name}")
            
            # å‡†å¤‡æµ‹è¯•æ•°æ®
            test_input = np.random.randn(1, 30, 33).astype(np.float32)
            
            # é¢„çƒ­
            for _ in range(5):
                _ = session.run([output_name], {input_name: test_input})
            
            # æ€§èƒ½æµ‹è¯•
            times = []
            outputs = []
            
            for i in range(self.num_runs):
                start_time = time.time()
                outputs_result = session.run([output_name], {input_name: test_input})
                end_time = time.time()
                times.append((end_time - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
                outputs.append(outputs_result[0][0])
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            times_array = np.array(times)
            outputs_array = np.array(outputs)
            
            result = {
                'avg_time': np.mean(times_array),
                'std_time': np.std(times_array),
                'min_time': np.min(times_array),
                'max_time': np.max(times_array),
                'output': np.mean(outputs_array),
                'output_std': np.std(outputs_array),
                'times': times_array,
                'file_size': os.path.getsize(onnx_path) / 1024 / 1024,
                'memory_usage': self.get_memory_usage()
            }
            
            print(f"   âœ… {model_name} æµ‹è¯•å®Œæˆ")
            print(f"   å¹³å‡æ¨ç†æ—¶é—´: {result['avg_time']:.3f} Â± {result['std_time']:.3f} ms")
            print(f"   æ—¶é—´èŒƒå›´: {result['min_time']:.3f} - {result['max_time']:.3f} ms")
            print(f"   è¾“å‡ºå€¼: {result['output']:.6f} Â± {result['output_std']:.6f}")
            print(f"   æ–‡ä»¶å¤§å°: {result['file_size']:.2f} MB")
            
            return result
            
        except Exception as e:
            print(f"âŒ {model_name} æµ‹è¯•å¤±è´¥: {e}")
            self.clear_memory()
            return None

    def test_tensorrt_engine(self, engine_path, model_name):
        """æµ‹è¯•TensorRTå¼•æ“æ¨ç†å»¶è¿Ÿ"""
        print(f"\nâš¡ æµ‹è¯• {model_name} (TensorRT)...")
        
        if not os.path.exists(engine_path):
            print(f"âŒ å¼•æ“æ–‡ä»¶ä¸å­˜åœ¨: {engine_path}")
            return None
        
        # æ¸…ç†GPUå†…å­˜
        self.clear_memory()
        
        try:
            import tensorrt as trt
            
            # åŠ è½½å¼•æ“
            TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
            runtime = trt.Runtime(TRT_LOGGER)
            
            with open(engine_path, 'rb') as f:
                engine_data = f.read()
            
            engine = runtime.deserialize_cuda_engine(engine_data)
            if engine is None:
                print(f"âŒ {model_name} å¼•æ“åŠ è½½å¤±è´¥")
                return None
            
            # åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡
            context = engine.create_execution_context()
            if context is None:
                print(f"âŒ {model_name} æ— æ³•åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡")
                return None
            
            # å‡†å¤‡æµ‹è¯•æ•°æ®
            input_data = np.random.randn(1, 30, 33).astype(np.float32)
            context.set_input_shape('input', input_data.shape)
            
            # è·å–è¾“å‡ºå½¢çŠ¶
            output_shape = context.get_tensor_shape('up_probability')
            output_data = np.empty(output_shape, dtype=np.float32)
            
            # è®¾ç½®å¼ é‡åœ°å€
            context.set_tensor_address('input', input_data.ctypes.data)
            context.set_tensor_address('up_probability', output_data.ctypes.data)
            
            # é¢„çƒ­
            for _ in range(10):
                context.execute_async_v3(0)
            
            # ç²¾ç¡®æµ‹é‡
            times = []
            outputs = []
            
            for _ in range(self.num_runs):
                start = time.time()
                success = context.execute_async_v3(0)
                end = time.time()
                
                if success:
                    times.append((end - start) * 1000)
                    outputs.append(output_data[0])
            
            if times:
                times_array = np.array(times)
                outputs_array = np.array(outputs)
                
                result = {
                    'avg_time': np.mean(times_array),
                    'std_time': np.std(times_array),
                    'min_time': np.min(times_array),
                    'max_time': np.max(times_array),
                    'output': np.mean(outputs_array),
                    'output_std': np.std(outputs_array),
                    'times': times_array,
                    'file_size': os.path.getsize(engine_path) / 1024 / 1024,
                    'memory_usage': self.get_memory_usage()
                }
                
                print(f"   âœ… {model_name} æµ‹è¯•å®Œæˆ")
                print(f"   å¹³å‡æ¨ç†æ—¶é—´: {result['avg_time']:.3f} Â± {result['std_time']:.3f} ms")
                print(f"   æ—¶é—´èŒƒå›´: {result['min_time']:.3f} - {result['max_time']:.3f} ms")
                print(f"   è¾“å‡ºå€¼: {result['output']:.6f} Â± {result['output_std']:.6f}")
                print(f"   æ–‡ä»¶å¤§å°: {result['file_size']:.2f} MB")
                
                return result
            else:
                print(f"âŒ {model_name} æ¨ç†å¤±è´¥")
                return None
                
        except Exception as e:
            print(f"âŒ {model_name} æµ‹è¯•å¤±è´¥: {e}")
            print("   è¿™å¯èƒ½æ˜¯ç”±äºTensorRTå¼•æ“çš„å†…å­˜è®¿é—®é—®é¢˜")
            self.clear_memory()
            return None

    def run_comparison_test(self):
        """è¿è¡Œå®Œæ•´çš„å¯¹æ¯”æµ‹è¯•"""
        print("=" * 80)
        print("ğŸš€ å¢å¼ºç‰ˆå››ç§æ¨¡å‹æ¨ªå‘å¯¹æ¯”æµ‹è¯•")
        print("=" * 80)
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
        if not self.check_model_files():
            print("\nâš ï¸  éƒ¨åˆ†æ¨¡å‹æ–‡ä»¶ç¼ºå¤±ï¼Œå°†è·³è¿‡ç›¸åº”æµ‹è¯•")
        
        # å‡†å¤‡å›ºå®šè¾“å…¥æ•°æ®
        print("\nğŸ“Š å‡†å¤‡å›ºå®šè¾“å…¥æ•°æ®...")
        test_input = torch.randn(1, 30, 33).to(self.device)
        print(f"   è¾“å…¥å½¢çŠ¶: {test_input.shape}")
        print(f"   æ•°æ®ç±»å‹: {test_input.dtype}")
        
        # 1. æµ‹è¯•åŸå§‹æ•™å¸ˆæ¨¡å‹ (PyTorch)
        print("\n" + "="*60)
        print("1ï¸âƒ£ åŸå§‹æ•™å¸ˆæ¨¡å‹ (PyTorch)")
        print("="*60)
        teacher_model = self.load_teacher_model()
        if teacher_model is not None:
            self.results['åŸå§‹æ¨¡å‹_PyTorch'] = self.test_pytorch_model(teacher_model, test_input, "åŸå§‹æ•™å¸ˆæ¨¡å‹")
            del teacher_model
            self.clear_memory()
        else:
            print("âš ï¸ è·³è¿‡åŸå§‹æ•™å¸ˆæ¨¡å‹æµ‹è¯•")
        
        # 2. æµ‹è¯•TensorRT FP32åçš„åŸå§‹æ¨¡å‹
        print("\n" + "="*60)
        print("2ï¸âƒ£ TensorRT FP32åçš„åŸå§‹æ¨¡å‹")
        print("="*60)
        self.results['åŸå§‹æ¨¡å‹_TensorRT_FP32'] = self.test_tensorrt_engine(
            self.model_paths['teacher_trt_fp32'], 
            "åŸå§‹æ¨¡å‹ TensorRT FP32"
        )
        
        # 3. æµ‹è¯•è’¸é¦å‰ªæè¿‡åçš„å­¦ç”Ÿæ¨¡å‹ (PyTorch)
        print("\n" + "="*60)
        print("3ï¸âƒ£ è’¸é¦å‰ªæè¿‡åçš„å­¦ç”Ÿæ¨¡å‹ (PyTorch)")
        print("="*60)
        student_model = self.load_student_model()
        if student_model is not None:
            self.results['å­¦ç”Ÿæ¨¡å‹_PyTorch'] = self.test_pytorch_model(student_model, test_input, "å­¦ç”Ÿæ¨¡å‹")
            del student_model
            self.clear_memory()
        else:
            print("âš ï¸ è·³è¿‡å­¦ç”Ÿæ¨¡å‹æµ‹è¯•")
        
        # 4. æµ‹è¯•è’¸é¦å‰ªæè¿‡åå†TensorRTç¼–è¯‘ä¼˜åŒ–çš„å­¦ç”Ÿæ¨¡å‹
        print("\n" + "="*60)
        print("4ï¸âƒ£ è’¸é¦å‰ªæè¿‡åå†TensorRTç¼–è¯‘ä¼˜åŒ–çš„å­¦ç”Ÿæ¨¡å‹")
        print("="*60)
        self.results['å­¦ç”Ÿæ¨¡å‹_TensorRT_FP32'] = self.test_tensorrt_engine(
            self.model_paths['student_trt_fp32'], 
            "å­¦ç”Ÿæ¨¡å‹ TensorRT FP32"
        )
        
        # 5. é¢å¤–æµ‹è¯•ï¼šå­¦ç”Ÿæ¨¡å‹TensorRT FP16
        print("\n" + "="*60)
        print("5ï¸âƒ£ å­¦ç”Ÿæ¨¡å‹ TensorRT FP16 (é¢å¤–æµ‹è¯•)")
        print("="*60)
        self.results['å­¦ç”Ÿæ¨¡å‹_TensorRT_FP16'] = self.test_tensorrt_engine(
            self.model_paths['student_trt_fp16'], 
            "å­¦ç”Ÿæ¨¡å‹ TensorRT FP16"
        )
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        self.generate_comparison_report()

    def generate_comparison_report(self):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ“Š å››ç§æ¨¡å‹æ¨ªå‘å¯¹æ¯”ç»“æœ")
        print("="*80)
        
        if not self.results:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•ç»“æœ")
            return
        
        # è¿‡æ»¤æ‰Noneç»“æœå¹¶æŒ‰æ¨ç†æ—¶é—´æ’åº
        valid_results = {k: v for k, v in self.results.items() if v is not None}
        sorted_results = sorted(valid_results.items(), key=lambda x: x[1]['avg_time'])
        
        # æ‰“å°è¯¦ç»†å¯¹æ¯”è¡¨
        print(f"{'æ’å':<4} {'æ¨¡å‹ç±»å‹':<25} {'å¹³å‡å»¶è¿Ÿ(ms)':<15} {'æ ‡å‡†å·®(ms)':<12} {'è¾“å‡ºå€¼':<12} {'æ–‡ä»¶å¤§å°(MB)':<12}")
        print("-" * 90)
        
        for i, (model_name, result) in enumerate(sorted_results, 1):
            file_size = result.get('file_size', 'N/A')
            if file_size != 'N/A':
                file_size = f"{file_size:.2f}"
            
            medal = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f"{i}."
            print(f"{medal:<4} {model_name:<25} {result['avg_time']:<15.3f} {result['std_time']:<12.3f} {result['output']:<12.6f} {file_size:<12}")
        
        # è®¡ç®—åŠ é€Ÿæ¯”
        if 'åŸå§‹æ¨¡å‹_PyTorch' in self.results and self.results['åŸå§‹æ¨¡å‹_PyTorch'] is not None:
            baseline_time = self.results['åŸå§‹æ¨¡å‹_PyTorch']['avg_time']
            print(f"\nğŸš€ åŠ é€Ÿæ¯”åˆ†æ (ä»¥åŸå§‹æ¨¡å‹PyTorchä¸ºåŸºå‡†):")
            print("-" * 60)
            
            for model_name, result in self.results.items():
                if model_name != 'åŸå§‹æ¨¡å‹_PyTorch' and result is not None:
                    speedup = baseline_time / result['avg_time']
                    print(f"{model_name:<30}: {speedup:.2f}x")
        
        # æ¨èæ–¹æ¡ˆ
        print(f"\nğŸ¯ æ¨èä½¿ç”¨æ–¹æ¡ˆ:")
        fastest_model = sorted_results[0]
        print(f"   ğŸ† æœ€å¿«æ¨ç†: {fastest_model[0]} ({fastest_model[1]['avg_time']:.3f} ms)")
        
        # å­¦ç”Ÿæ¨¡å‹æœ€ä½³
        student_models = [(name, result) for name, result in self.results.items() if 'å­¦ç”Ÿæ¨¡å‹' in name and result is not None]
        if student_models:
            fastest_student = min(student_models, key=lambda x: x[1]['avg_time'])
            print(f"   ğŸ“ å­¦ç”Ÿæ¨¡å‹æœ€ä½³: {fastest_student[0]} ({fastest_student[1]['avg_time']:.3f} ms)")
        
        # ä½“ç§¯æœ€å°
        models_with_size = [(name, result) for name, result in self.results.items() if result is not None and 'file_size' in result]
        if models_with_size:
            smallest_model = min(models_with_size, key=lambda x: x[1]['file_size'])
            print(f"   ğŸ’¾ æœ€å°ä½“ç§¯: {smallest_model[0]} ({smallest_model[1]['file_size']:.2f} MB)")
        
        # ä¿å­˜æŠ¥å‘Š
        if self.save_report:
            self.save_detailed_report()

    def save_detailed_report(self):
        """ä¿å­˜è¯¦ç»†æŠ¥å‘Š"""
        report_data = {
            'test_info': {
                'timestamp': self.start_time.isoformat(),
                'device': self.device,
                'num_runs': self.num_runs,
                'test_duration': (datetime.now() - self.start_time).total_seconds()
            },
            'results': {}
        }
        
        # å¤„ç†ç»“æœæ•°æ®ï¼ˆç§»é™¤numpyæ•°ç»„ï¼‰
        for model_name, result in self.results.items():
            if result is not None:
                clean_result = {}
                for key, value in result.items():
                    if isinstance(value, np.ndarray):
                        clean_result[key] = value.tolist()
                    elif isinstance(value, dict):
                        clean_result[key] = value
                    else:
                        clean_result[key] = float(value) if isinstance(value, (int, float)) else value
                report_data['results'][model_name] = clean_result
        
        # ä¿å­˜JSONæŠ¥å‘Š
        report_file = f"model_comparison_report_{self.start_time.strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        self.generate_html_report(report_data)

    def generate_html_report(self, report_data):
        """ç”ŸæˆHTMLæ ¼å¼çš„å¯¹æ¯”æŠ¥å‘Š"""
        html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ¨¡å‹å¯¹æ¯”æµ‹è¯•æŠ¥å‘Š</title>
    <style>
        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 20px; background-color: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 20px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        h1 {{ color: #2c3e50; text-align: center; margin-bottom: 30px; }}
        h2 {{ color: #34495e; border-bottom: 2px solid #3498db; padding-bottom: 10px; }}
        .summary {{ background: #ecf0f1; padding: 15px; border-radius: 5px; margin: 20px 0; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
        th {{ background-color: #3498db; color: white; }}
        tr:nth-child(even) {{ background-color: #f2f2f2; }}
        .fastest {{ background-color: #d4edda !important; }}
        .recommendation {{ background: #e8f5e8; padding: 15px; border-left: 4px solid #28a745; margin: 15px 0; }}
        .metric {{ display: inline-block; margin: 5px 10px; padding: 5px 10px; background: #3498db; color: white; border-radius: 3px; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸš€ å››ç§æ¨¡å‹æ¨ªå‘å¯¹æ¯”æµ‹è¯•æŠ¥å‘Š</h1>
        
        <div class="summary">
            <h3>ğŸ“Š æµ‹è¯•æ¦‚è§ˆ</h3>
            <p><strong>æµ‹è¯•æ—¶é—´:</strong> {report_data['test_info']['timestamp']}</p>
            <p><strong>æµ‹è¯•è®¾å¤‡:</strong> {report_data['test_info']['device']}</p>
            <p><strong>æµ‹è¯•æ¬¡æ•°:</strong> {report_data['test_info']['num_runs']}</p>
            <p><strong>æµ‹è¯•æ—¶é•¿:</strong> {report_data['test_info']['test_duration']:.2f} ç§’</p>
        </div>
        
        <h2>ğŸ“ˆ æ€§èƒ½å¯¹æ¯”è¡¨</h2>
        <table>
            <thead>
                <tr>
                    <th>æ’å</th>
                    <th>æ¨¡å‹ç±»å‹</th>
                    <th>å¹³å‡å»¶è¿Ÿ(ms)</th>
                    <th>æ ‡å‡†å·®(ms)</th>
                    <th>è¾“å‡ºå€¼</th>
                    <th>æ–‡ä»¶å¤§å°(MB)</th>
                </tr>
            </thead>
            <tbody>
"""
        
        # æ’åºç»“æœ
        valid_results = {k: v for k, v in report_data['results'].items() if v is not None}
        sorted_results = sorted(valid_results.items(), key=lambda x: x[1]['avg_time'])
        
        for i, (model_name, result) in enumerate(sorted_results, 1):
            file_size = result.get('file_size', 'N/A')
            if file_size != 'N/A':
                file_size = f"{file_size:.2f}"
            
            medal = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f"{i}."
            row_class = "fastest" if i == 1 else ""
            
            html_content += f"""
                <tr class="{row_class}">
                    <td>{medal}</td>
                    <td>{model_name}</td>
                    <td>{result['avg_time']:.3f}</td>
                    <td>{result['std_time']:.3f}</td>
                    <td>{result['output']:.6f}</td>
                    <td>{file_size}</td>
                </tr>
"""
        
        html_content += """
            </tbody>
        </table>
        
        <div class="recommendation">
            <h3>ğŸ¯ æ¨èä½¿ç”¨æ–¹æ¡ˆ</h3>
"""
        
        if sorted_results:
            fastest_model = sorted_results[0]
            html_content += f"""
            <p><strong>ğŸ† æœ€å¿«æ¨ç†:</strong> {fastest_model[0]} ({fastest_model[1]['avg_time']:.3f} ms)</p>
"""
            
            # å­¦ç”Ÿæ¨¡å‹æœ€ä½³
            student_models = [(name, result) for name, result in report_data['results'].items() if 'å­¦ç”Ÿæ¨¡å‹' in name and result is not None]
            if student_models:
                fastest_student = min(student_models, key=lambda x: x[1]['avg_time'])
                html_content += f"""
            <p><strong>ğŸ“ å­¦ç”Ÿæ¨¡å‹æœ€ä½³:</strong> {fastest_student[0]} ({fastest_student[1]['avg_time']:.3f} ms)</p>
"""
            
            # ä½“ç§¯æœ€å°
            models_with_size = [(name, result) for name, result in report_data['results'].items() if result is not None and 'file_size' in result]
            if models_with_size:
                smallest_model = min(models_with_size, key=lambda x: x[1]['file_size'])
                html_content += f"""
            <p><strong>ğŸ’¾ æœ€å°ä½“ç§¯:</strong> {smallest_model[0]} ({smallest_model[1]['file_size']:.2f} MB)</p>
"""
        
        html_content += """
        </div>
        
        <h2>ğŸ“Š åŠ é€Ÿæ¯”åˆ†æ</h2>
        <div class="summary">
"""
        
        # è®¡ç®—åŠ é€Ÿæ¯”
        if 'åŸå§‹æ¨¡å‹_PyTorch' in report_data['results'] and report_data['results']['åŸå§‹æ¨¡å‹_PyTorch'] is not None:
            baseline_time = report_data['results']['åŸå§‹æ¨¡å‹_PyTorch']['avg_time']
            html_content += f"<p><strong>åŸºå‡†æ¨¡å‹:</strong> åŸå§‹æ¨¡å‹_PyTorch ({baseline_time:.3f} ms)</p>"
            
            for model_name, result in report_data['results'].items():
                if model_name != 'åŸå§‹æ¨¡å‹_PyTorch' and result is not None:
                    speedup = baseline_time / result['avg_time']
                    html_content += f"<p><strong>{model_name}:</strong> {speedup:.2f}x åŠ é€Ÿ</p>"
        
        html_content += """
        </div>
        
        <div class="summary">
            <h3>âœ… æµ‹è¯•å®Œæˆ</h3>
            <p>æœ¬æ¬¡æµ‹è¯•å¯¹æ¯”äº†å››ç§ä¸åŒçš„æ¨¡å‹ä¼˜åŒ–æ–¹æ¡ˆï¼Œä¸ºç”Ÿäº§éƒ¨ç½²æä¾›äº†å¤šç§é€‰æ‹©ã€‚</p>
            <p>å»ºè®®æ ¹æ®å…·ä½“åº”ç”¨åœºæ™¯é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹ç‰ˆæœ¬ã€‚</p>
        </div>
    </div>
</body>
</html>
"""
        
        # ä¿å­˜HTMLæŠ¥å‘Š
        html_file = f"model_comparison_report_{self.start_time.strftime('%Y%m%d_%H%M%S')}.html"
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"ğŸŒ HTMLæŠ¥å‘Šå·²ä¿å­˜: {html_file}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆå››ç§æ¨¡å‹æ¨ªå‘å¯¹æ¯”æµ‹è¯•')
    parser.add_argument('--device', type=str, default='cuda', choices=['cuda', 'cpu'], help='æµ‹è¯•è®¾å¤‡')
    parser.add_argument('--num_runs', type=int, default=200, help='æµ‹è¯•æ¬¡æ•°')
    parser.add_argument('--no_report', action='store_true', help='ä¸ä¿å­˜æŠ¥å‘Š')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = ModelComparisonTester(
        device=args.device,
        num_runs=args.num_runs,
        save_report=not args.no_report
    )
    
    # è¿è¡Œå¯¹æ¯”æµ‹è¯•
    tester.run_comparison_test()
    
    print(f"\nâœ… å¢å¼ºç‰ˆå››ç§æ¨¡å‹æ¨ªå‘å¯¹æ¯”æµ‹è¯•å®Œæˆ!")
    print(f"   æµ‹è¯•æ¬¡æ•°: {args.num_runs}æ¬¡å–å¹³å‡")
    print(f"   è¾“å…¥æ•°æ®: å›ºå®šéšæœºè¾“å…¥ (1, 30, 33)")
    print(f"   æµ‹è¯•è®¾å¤‡: {args.device}")

if __name__ == "__main__":
    main()
