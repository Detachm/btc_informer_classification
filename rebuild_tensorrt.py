#!/usr/bin/env python3
"""
é‡æ–°æ„å»ºTensorRTå¼•æ“ - ä½¿ç”¨æ›´ä¿å®ˆçš„è®¾ç½®
"""

import os
import tensorrt as trt
import numpy as np

def rebuild_tensorrt_engine(onnx_path="informer_cls.optimized.onnx", engine_path="informer_cls.trt.fp32.engine", precision="fp32"):
    """é‡æ–°æ„å»ºTensorRTå¼•æ“ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„è®¾ç½®"""
    print("=" * 60)
    print("é‡æ–°æ„å»ºTensorRTå¼•æ“")
    print("=" * 60)
    
    print(f"è¾“å…¥ONNX: {onnx_path}")
    print(f"è¾“å‡ºå¼•æ“: {engine_path}")
    print(f"ç²¾åº¦æ¨¡å¼: {precision}")
    
    if not os.path.exists(onnx_path):
        print(f"âŒ æ‰¾ä¸åˆ°ONNXæ¨¡å‹: {onnx_path}")
        return False
    
    try:
        # 1. åˆ›å»ºTensorRTæ—¥å¿—è®°å½•å™¨
        print("1. åˆ›å»ºTensorRTæ—¥å¿—è®°å½•å™¨...")
        TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
        
        # 2. åˆ›å»ºæ„å»ºå™¨å’Œç½‘ç»œ
        print("2. åˆ›å»ºæ„å»ºå™¨å’Œç½‘ç»œ...")
        builder = trt.Builder(TRT_LOGGER)
        network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
        
        # 3. åˆ›å»ºONNXè§£æå™¨
        print("3. åˆ›å»ºONNXè§£æå™¨...")
        parser = trt.OnnxParser(network, TRT_LOGGER)
        
        # 4. è§£æONNXæ¨¡å‹
        print("4. è§£æONNXæ¨¡å‹...")
        with open(onnx_path, 'rb') as model:
            if not parser.parse(model.read()):
                print("âŒ ONNXè§£æå¤±è´¥:")
                for error in range(parser.num_errors):
                    print(f"   {parser.get_error(error)}")
                return False
        
        print("âœ… ONNXè§£ææˆåŠŸ!")
        
        # 5. åˆ›å»ºæ„å»ºé…ç½®
        print("5. åˆ›å»ºæ„å»ºé…ç½®...")
        config = builder.create_builder_config()
        
        # è®¾ç½®å†…å­˜æ± é™åˆ¶ï¼ˆTensorRT 10.xï¼‰
        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)  # 1GB
        
        # è®¾ç½®ç²¾åº¦æ¨¡å¼
        if precision == "fp16":
            print("   ä½¿ç”¨FP16ç²¾åº¦...")
            config.set_flag(trt.BuilderFlag.FP16)
        elif precision == "int8":
            print("   ä½¿ç”¨INT8ç²¾åº¦...")
            config.set_flag(trt.BuilderFlag.INT8)
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ ¡å‡†æ•°æ®é›†
        else:
            print("   ä½¿ç”¨FP32ç²¾åº¦...")
        
        # 6. è®¾ç½®ä¼˜åŒ–é…ç½®æ–‡ä»¶
        print("6. è®¾ç½®ä¼˜åŒ–é…ç½®æ–‡ä»¶...")
        profile = builder.create_optimization_profile()
        
        # è®¾ç½®è¾“å…¥å½¢çŠ¶èŒƒå›´
        input_name = 'input'
        profile.set_shape(input_name, (1, 30, 33), (1, 30, 33), (1, 30, 33))  # å›ºå®šbatch=1
        config.add_optimization_profile(profile)
        
        print(f"   è¾“å…¥å½¢çŠ¶: (1, 30, 33)")
        
        # 7. æ„å»ºå¼•æ“
        print("7. æ„å»ºTensorRTå¼•æ“...")
        print("   è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")
        
        serialized_engine = builder.build_serialized_network(network, config)
        
        if serialized_engine is None:
            print("âŒ å¼•æ“æ„å»ºå¤±è´¥")
            return False
        
        # 8. ä¿å­˜å¼•æ“
        print("8. ä¿å­˜å¼•æ“...")
        with open(engine_path, 'wb') as f:
            f.write(serialized_engine)
        
        print(f"âœ… å¼•æ“æ„å»ºæˆåŠŸ!")
        print(f"   æ–‡ä»¶è·¯å¾„: {os.path.abspath(engine_path)}")
        print(f"   æ–‡ä»¶å¤§å°: {os.path.getsize(engine_path) / 1024 / 1024:.2f} MB")
        
        # 9. éªŒè¯å¼•æ“
        print("\n9. éªŒè¯å¼•æ“...")
        runtime = trt.Runtime(TRT_LOGGER)
        engine = runtime.deserialize_cuda_engine(serialized_engine)
        
        if engine is None:
            print("âŒ å¼•æ“éªŒè¯å¤±è´¥")
            return False
        
        print(f"âœ… å¼•æ“éªŒè¯æˆåŠŸ!")
        print(f"   IOå¼ é‡æ•°é‡: {engine.num_io_tensors}")
        print(f"   å±‚æ•°: {engine.num_layers}")
        
        # æ˜¾ç¤ºå¼ é‡ä¿¡æ¯
        for i in range(engine.num_io_tensors):
            name = engine.get_tensor_name(i)
            mode = engine.get_tensor_mode(name)
            shape = engine.get_tensor_shape(name)
            dtype = engine.get_tensor_dtype(name)
            
            print(f"   {i}: {name} - {mode} - å½¢çŠ¶: {shape} - ç±»å‹: {dtype}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ„å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_new_engine(engine_path="informer_cls.trt.fp32.engine"):
    """æµ‹è¯•æ–°æ„å»ºçš„å¼•æ“"""
    print(f"\nğŸ§ª æµ‹è¯•æ–°æ„å»ºçš„å¼•æ“:")
    
    if not os.path.exists(engine_path):
        print(f"âŒ æ‰¾ä¸åˆ°å¼•æ“æ–‡ä»¶: {engine_path}")
        return False
    
    try:
        # åŠ è½½å¼•æ“
        TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
        runtime = trt.Runtime(TRT_LOGGER)
        
        with open(engine_path, 'rb') as f:
            engine_data = f.read()
        
        engine = runtime.deserialize_cuda_engine(engine_data)
        if engine is None:
            print("âŒ å¼•æ“åŠ è½½å¤±è´¥")
            return False
        
        print("âœ… å¼•æ“åŠ è½½æˆåŠŸ!")
        
        # åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡
        context = engine.create_execution_context()
        if context is None:
            print("âŒ æ— æ³•åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡")
            return False
        
        print("âœ… æ‰§è¡Œä¸Šä¸‹æ–‡åˆ›å»ºæˆåŠŸ!")
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        input_data = np.random.randn(1, 30, 33).astype(np.float32)
        context.set_input_shape('input', input_data.shape)
        
        # è·å–è¾“å‡ºå½¢çŠ¶
        output_shape = context.get_tensor_shape('up_probability')
        output_data = np.empty(output_shape, dtype=np.float32)
        
        print(f"   è¾“å…¥å½¢çŠ¶: {input_data.shape}")
        print(f"   è¾“å‡ºå½¢çŠ¶: {output_shape}")
        
        # è®¾ç½®å¼ é‡åœ°å€
        context.set_tensor_address('input', input_data.ctypes.data)
        context.set_tensor_address('up_probability', output_data.ctypes.data)
        
        # æ‰§è¡Œæ¨ç†
        import time
        start_time = time.time()
        success = context.execute_async_v3(0)
        end_time = time.time()
        
        if not success:
            print("âŒ æ¨ç†å¤±è´¥")
            return False
        
        inference_time = end_time - start_time
        print(f"âœ… æ¨ç†æˆåŠŸ!")
        print(f"   æ¨ç†æ—¶é—´: {inference_time*1000:.2f} ms")
        print(f"   è¾“å‡ºå€¼: {output_data[0]:.6f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ„å»ºTensorRTå¼•æ“')
    parser.add_argument('--onnx_path', type=str, default='informer_cls.optimized.onnx', help='ONNXæ¨¡å‹è·¯å¾„')
    parser.add_argument('--engine_path', type=str, default='informer_cls.trt.fp32.engine', help='å¼•æ“è¾“å‡ºè·¯å¾„')
    parser.add_argument('--precision', type=str, default='fp32', choices=['fp32', 'fp16', 'int8'], help='ç²¾åº¦æ¨¡å¼')
    args = parser.parse_args()
    
    # é‡æ–°æ„å»ºå¼•æ“
    success = rebuild_tensorrt_engine(args.onnx_path, args.engine_path, args.precision)
    
    if success:
        # æµ‹è¯•æ–°å¼•æ“
        test_success = test_new_engine(args.engine_path)
        
        if test_success:
            print(f"\nğŸ‰ TensorRTå¼•æ“é‡å»ºå’Œæµ‹è¯•æˆåŠŸï¼")
        else:
            print(f"\nâš ï¸  å¼•æ“æ„å»ºæˆåŠŸä½†æµ‹è¯•å¤±è´¥")
    else:
        print(f"\nâŒ TensorRTå¼•æ“é‡å»ºå¤±è´¥")
    
    print("=" * 60)

if __name__ == '__main__':
    main()
