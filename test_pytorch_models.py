#!/usr/bin/env python3
"""
æµ‹è¯•åŸå§‹æ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹çš„PyTorchç‰ˆæœ¬
è·å–åŸºå‡†æ€§èƒ½æ•°æ®
"""

import torch
import numpy as np
import time
import os
import warnings
import gc
warnings.filterwarnings('ignore')

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from InformerClassification import InformerClassification, InformerConfig, create_informer_classification_model
from knowledge_distillation_training import StudentInformerClassification, StudentInformerConfig

def clear_cuda_memory():
    """å®‰å…¨æ¸…ç†CUDAå†…å­˜"""
    try:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
    except Exception as e:
        print(f"CUDAå†…å­˜æ¸…ç†è­¦å‘Š: {e}")
    gc.collect()

def load_teacher_model(device='cuda'):
    """åŠ è½½åŸå§‹æ•™å¸ˆæ¨¡å‹"""
    print("ğŸ“š åŠ è½½åŸå§‹æ•™å¸ˆæ¨¡å‹...")
    teacher_model_path = 'checkpoints/bitcoin_mega_classification/mega_best_model.pth'
    
    if not os.path.exists(teacher_model_path):
        print(f"âŒ æ•™å¸ˆæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {teacher_model_path}")
        return None
    
    try:
        # æ¸…ç†å†…å­˜
        clear_cuda_memory()
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        checkpoint = torch.load(teacher_model_path, map_location=device)
        config_dict = checkpoint['config']
        
        # åˆ›å»ºé…ç½®
        teacher_config = InformerConfig()
        for key, value in config_dict.items():
            if hasattr(teacher_config, key):
                setattr(teacher_config, key, value)
        
        # åˆ›å»ºæ¨¡å‹
        teacher_model, _ = create_informer_classification_model(teacher_config)
        teacher_model = teacher_model.to(device)
        
        # å¤„ç†åŠ¨æ€æŠ•å½±å±‚
        model_state_dict = checkpoint['model_state_dict']
        if 'projection.weight' in model_state_dict:
            dummy_input = torch.randn(1, teacher_config.seq_len, teacher_config.enc_in).to(device)
            with torch.no_grad():
                _ = teacher_model(dummy_input)
        
        teacher_model.load_state_dict(model_state_dict, strict=False)
        teacher_model.eval()
        
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in teacher_model.parameters())
        print(f"   âœ… æ•™å¸ˆæ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"   å‚æ•°é‡: {total_params:,} ({total_params/1_000_000:.1f}M)")
        
        return teacher_model
        
    except Exception as e:
        print(f"âŒ æ•™å¸ˆæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        clear_cuda_memory()
        return None

def load_student_model(device='cuda'):
    """åŠ è½½å­¦ç”Ÿæ¨¡å‹"""
    print("ğŸ“ åŠ è½½å­¦ç”Ÿæ¨¡å‹...")
    student_model_path = 'checkpoints/student_distillation/student_best_model.pth'
    
    if not os.path.exists(student_model_path):
        print(f"âŒ å­¦ç”Ÿæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {student_model_path}")
        return None
    
    try:
        # æ¸…ç†å†…å­˜
        clear_cuda_memory()
        
        # åŠ è½½æ£€æŸ¥ç‚¹ - ä½¿ç”¨CPUåŠ è½½é¿å…CUDAé”™è¯¯
        checkpoint = torch.load(student_model_path, map_location='cpu')
        config_dict = checkpoint['config']
        
        # åˆ›å»ºå­¦ç”Ÿé…ç½®
        student_config = StudentInformerConfig()
        for key, value in config_dict.items():
            if hasattr(student_config, key):
                setattr(student_config, key, value)
        
        # åˆ›å»ºå­¦ç”Ÿæ¨¡å‹
        student_model = StudentInformerClassification(student_config)
        
        # å¤„ç†åŠ¨æ€æŠ•å½±å±‚ï¼ˆåœ¨CPUä¸Šï¼‰
        model_state_dict = checkpoint['model_state_dict']
        if 'projection.weight' in model_state_dict:
            dummy_input = torch.randn(1, student_config.seq_len, student_config.enc_in)
            with torch.no_grad():
                _ = student_model(dummy_input)
        
        # åŠ è½½æƒé‡
        student_model.load_state_dict(model_state_dict, strict=False)
        student_model.eval()
        
        # ç§»åŠ¨åˆ°GPUï¼ˆå¦‚æœéœ€è¦ï¼‰
        if device == 'cuda':
            student_model = student_model.to(device)
            clear_cuda_memory()
    
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in student_model.parameters())
        print(f"   âœ… å­¦ç”Ÿæ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"   å‚æ•°é‡: {total_params:,} ({total_params/1_000_000:.1f}M)")
        
        return student_model
        
    except Exception as e:
        print(f"âŒ å­¦ç”Ÿæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        clear_cuda_memory()
        return None

def test_pytorch_model(model, test_input, model_name, num_runs=200):
    """æµ‹è¯•PyTorchæ¨¡å‹æ¨ç†å»¶è¿Ÿ"""
    print(f"\nğŸ§ª æµ‹è¯• {model_name} (PyTorch)...")
    
    # é¢„çƒ­
    with torch.no_grad():
        for _ in range(10):
            _ = model(test_input)
    
    # ç²¾ç¡®æµ‹é‡
    times = []
    with torch.no_grad():
        for i in range(num_runs):
            if torch.cuda.is_available():
                torch.cuda.synchronize()
            
            start = time.time()
            output = model(test_input)
            up_prob = output['up_probability']
            
            if torch.cuda.is_available():
                torch.cuda.synchronize()
            
            end = time.time()
            times.append((end - start) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
    
    times_array = np.array(times)
    avg_time = np.mean(times_array)
    std_time = np.std(times_array)
    
    print(f"   âœ… {model_name} æµ‹è¯•å®Œæˆ")
    print(f"   å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.3f} Â± {std_time:.3f} ms")
    print(f"   è¾“å‡ºå€¼: {up_prob[0].item():.6f}")
    
    return {
        'avg_time': avg_time,
        'std_time': std_time,
        'output': up_prob[0].item(),
        'times': times_array
    }

def main():
    """ä¸»å‡½æ•° - æµ‹è¯•PyTorchæ¨¡å‹æ€§èƒ½"""
    print("=" * 80)
    print("ğŸš€ PyTorchæ¨¡å‹æ€§èƒ½æµ‹è¯• - è·å–åŸºå‡†æ€§èƒ½æ•°æ®")
    print("=" * 80)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ¸…ç†GPUå†…å­˜
    clear_cuda_memory()
    
    # å‡†å¤‡å›ºå®šè¾“å…¥æ•°æ®
    print("\nğŸ“Š å‡†å¤‡å›ºå®šè¾“å…¥æ•°æ®...")
    test_input = torch.randn(1, 30, 33).to(device)
    print(f"è¾“å…¥å½¢çŠ¶: {test_input.shape}")
    
    results = {}
    
    # 1. æµ‹è¯•åŸå§‹æ•™å¸ˆæ¨¡å‹ (PyTorch)
    print("\n" + "="*60)
    print("1ï¸âƒ£ åŸå§‹æ•™å¸ˆæ¨¡å‹ (PyTorch)")
    print("="*60)
    teacher_model = load_teacher_model(device)
    if teacher_model is not None:
        results['åŸå§‹æ¨¡å‹_PyTorch'] = test_pytorch_model(teacher_model, test_input, "åŸå§‹æ•™å¸ˆæ¨¡å‹")
        del teacher_model
        clear_cuda_memory()
    else:
        print("âš ï¸ è·³è¿‡åŸå§‹æ•™å¸ˆæ¨¡å‹æµ‹è¯•")
    
    # 2. æµ‹è¯•å­¦ç”Ÿæ¨¡å‹ (PyTorch)
    print("\n" + "="*60)
    print("2ï¸âƒ£ å­¦ç”Ÿæ¨¡å‹ (PyTorch)")
    print("="*60)
    student_model = load_student_model(device)
    if student_model is not None:
        results['å­¦ç”Ÿæ¨¡å‹_PyTorch'] = test_pytorch_model(student_model, test_input, "å­¦ç”Ÿæ¨¡å‹")
        del student_model
        clear_cuda_memory()
    else:
        print("âš ï¸ è·³è¿‡å­¦ç”Ÿæ¨¡å‹æµ‹è¯•")
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    print("\n" + "="*80)
    print("ğŸ“Š PyTorchæ¨¡å‹æ€§èƒ½å¯¹æ¯”ç»“æœ")
    print("="*80)
    
    if results:
        # è¿‡æ»¤æ‰Noneç»“æœå¹¶æŒ‰æ¨ç†æ—¶é—´æ’åº
        valid_results = {k: v for k, v in results.items() if v is not None}
        sorted_results = sorted(valid_results.items(), key=lambda x: x[1]['avg_time'])
        
        print(f"{'æ’å':<4} {'æ¨¡å‹ç±»å‹':<25} {'å¹³å‡å»¶è¿Ÿ(ms)':<15} {'æ ‡å‡†å·®(ms)':<12} {'è¾“å‡ºå€¼':<12}")
        print("-" * 80)
        
        for i, (model_name, result) in enumerate(sorted_results, 1):
            medal = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else f"{i}."
            print(f"{medal:<4} {model_name:<25} {result['avg_time']:<15.3f} {result['std_time']:<12.3f} {result['output']:<12.6f}")
        
        # è®¡ç®—åŠ é€Ÿæ¯”
        if 'åŸå§‹æ¨¡å‹_PyTorch' in results and results['åŸå§‹æ¨¡å‹_PyTorch'] is not None:
            baseline_time = results['åŸå§‹æ¨¡å‹_PyTorch']['avg_time']
            print(f"\nğŸš€ åŠ é€Ÿæ¯”åˆ†æ (ä»¥åŸå§‹æ¨¡å‹PyTorchä¸ºåŸºå‡†):")
            print("-" * 60)
            
            for model_name, result in results.items():
                if model_name != 'åŸå§‹æ¨¡å‹_PyTorch' and result is not None:
                    speedup = baseline_time / result['avg_time']
                    print(f"{model_name:<30}: {speedup:.2f}x")
        
        # æ¨èæ–¹æ¡ˆ
        print(f"\nğŸ¯ PyTorchæ¨¡å‹æ¨è:")
        fastest_model = sorted_results[0]
        print(f"   ğŸ† æœ€å¿«æ¨ç†: {fastest_model[0]} ({fastest_model[1]['avg_time']:.3f} ms)")
        
        # å­¦ç”Ÿæ¨¡å‹ä¿¡æ¯
        if 'å­¦ç”Ÿæ¨¡å‹_PyTorch' in results and results['å­¦ç”Ÿæ¨¡å‹_PyTorch'] is not None:
            student_result = results['å­¦ç”Ÿæ¨¡å‹_PyTorch']
            print(f"   ğŸ“ å­¦ç”Ÿæ¨¡å‹: {student_result['avg_time']:.3f} ms")
    
    print(f"\nâœ… PyTorchæ¨¡å‹æ€§èƒ½æµ‹è¯•å®Œæˆ!")
    print(f"   æµ‹è¯•æ¬¡æ•°: 200æ¬¡å–å¹³å‡")
    print(f"   è¾“å…¥æ•°æ®: å›ºå®šéšæœºè¾“å…¥ (1, 30, 33)")
    print(f"   æµ‹è¯•è®¾å¤‡: {device}")
    
    # ä¸‹ä¸€æ­¥æŒ‡å¯¼
    print(f"\nğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ:")
    print(f"   1. æµ‹è¯•TensorRTå¼•æ“: python rebuild_tensorrt.py --onnx_path informer_cls.fp32.onnx --engine_path informer_cls.trt.fp32.engine --precision fp32")
    print(f"   2. æµ‹è¯•å­¦ç”Ÿæ¨¡å‹TensorRT: python rebuild_tensorrt.py --onnx_path student_model.fp32.onnx --engine_path student_model.trt.fp32.engine --precision fp32")

if __name__ == "__main__":
    main()
